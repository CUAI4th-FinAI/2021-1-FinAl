{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0404_ch05",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPlSxcJvVwKmpBk9U13jsd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CUAI4th-FinAI/2021-1-FinAl/blob/main/2021-1-FinAl/yamuzin-oksusu/0404_ch05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KblIPI280tYG"
      },
      "source": [
        "# CH05 오차역전파법\n",
        "\n",
        "계산 시간이 오래 걸린다는 수치 미분의 단점을 극복함<br> \n",
        "**가중치 매개 변수의 기울기를 효율적으로 계산 가능**<br>\n",
        "계산 그래프 혹은 수식을 중심으로 이를 이해 할 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NHHF7bP1Z9X"
      },
      "source": [
        "### 5.1 계산 그래프\n",
        "\n",
        "계산 과정을 그래프로 나타냄\n",
        "\n",
        "![graph.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAkMAAADuCAAAAAAfT1xeAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAJcEhZcwAAFiUAABYlAUlSJPAAAAAHdElNRQfiCQwUEgpTkF+BAAAycUlEQVR42u19eXxcV3X/99z73mza98WSJdvyvtuxHdtx4iwkIRhIAyRQQoBQ2gJl+9ECP3608EtLKYH+gBKaUlqgSdiXUtYkkMR27Nhx4sj7Lq9arX20zPLevef3x0i2NJqRZqSRPLLe9/OxPZ737vLe/c6555577jnEcOBgQhDXugMOpj0cDjmYKIypbY41iOhaP7SDlIKmVB/SAgDYIdF1hSmdy1g8tqLypl9PLW8dTDamci7T4v3/8dSC5970k7c5kuh6wlTKBG4rfnk98OnfHlbyWj+3g9RhKucy8n17nbZwofxaP7SDlGKKdRPW8p//+uhiZyq7njC1a3stQq97ee8SR6e+rjDF67KdGeW9GxyN+vrClM5lfGjVd9+jBBwOXVeYSg4pWfyXj1zrB3aQclzhUJITzJDVuU548+Lgqv8iAXH5o9rZpruOECWHzhZmQ0e+EnxyMRO0AJRkOleQHbmFAYCAUP28yBf9hiuhpph++ZhiwOh4zZ7ibToHk4kBDjE9U7IKSub9y7uuXOrJZttganox5y7YRs3fPTS0XPnF3bcpJQElFn3m3Y6WPJMxIBC0/MzrVzFQ6AU+eYAA4fl3rwFo8al/3dj8wL55KPRG7qz7usnem+9qZlk4ULLuBHRiducBCceOFLqucGU4c3wAoBh4660Ebd37HbYB8cLX/G48dUMn1MCk584APZZxF4MVAK3Nd7/+G++rsWUiksjRgq5LXOFQSAEAmLEeANpETi+0bXz3g4ayH3zo1OLB+yq+CDy5cUAtgrDf0rhn+4Jf3Hutn8PBtcOAaGCcvcAMINynAQB1PqEhPCi8ILWpOf9qCVZ99auBrh4CLj2cV7lHbT3+8ervasf4PFMxwCHZ2vwb0kD+nxf+DrjteRydA4W3vx+f+uWTaLjjTSVDi/x4ea6NFe8sAOnqxq9pyQvPfeey4xQ0YxGZy7T4f39y8Cf32+j43lukko0BvDwPEn9icMmhh9+T9a5vWOaVEsr4q+e0gYa9b0af/EhXFxFYL1rdUAEA6EueSpRxrV+CgwnBAACmi//UdPrmO7MADa3A2r6nTEk8ANbL9mGYRdE23nnXxrALlg3sea+HWZwtzAKj+GUlwXQ+eQ4FjVWObWA6wwAAaqv5emnp50uP1LCUcMFdZLwZYYtYiYAKa7JCHS45uC7/xIvnlACkAO5sAIA5P9gIAJAAYek4+nD0xCKHRNMYBgCun/0PH9H8OXNLk2Y8/dnMw28VNul/cJHx7HtkSJgG5M2mBgAtth2rg7QjhW0ASoY7bKYJeJHw0v0XZ1/r9+Bg/DAAUMXRJVpAf+YzMAgrvyyy7Zwsr9d+P+w76wdtOuskAAh8tYYFAANEIAmQJGlMSIwQr93tK7zWL8LBuBGZy5ZoAQhtu4IKZWUAoJnDDDBHzEbKsCIfMD+yX2pDBweqCE50RUa8+fn1mdf6TTgYL8SQf4SB++YNWA+FNMQDIJIRGOLu2UNKGOgwSx4AASC8o2iinSC+ZW/IsQ1MV4zXrJNiJZiDe251XNOmKaK2sIZZm1XcKwCBdewr4wJ51+x0rJTTFOkzcM3nb3RW+NMSabOVzqVFB9KH0A6SQNpwiHieccoh0XRE2nAIxMu66p3JbBoifTgE4vXn2q91JxwkjzTiEIi3HOq91p1wkDTSiUMg3rLXclSi6Ya04hBIbn7R0aunG9KLQyDv6h0OiaYZ0oxDQN6Clx0STS+kHYe4rOCQQ6JphbTjEHENnXFINJ2QdhwC8fK2BoLDommD9OMQiG+s6wDpidfkYEqQlrMG4/kNR7KWDtvGj/TT2QtJQ6ShHAKIbvlNld+68n8GQEREBHA6kn5mIy3lEMI7Vh1f2rCCCRGPyVB3R8gCpCsv1+fkakg3pGcYFyq+tKSOrciBgdCFVjO7yOMCrHDXsbBvbrbDorRCesohoPWEp7NyMZiCh62qssiMG2FOR11wSYFDojRCmnKICUfrjNezONaxuAB8VZdmEMIH1TrpsChtMDUc4sEVlZIDhFByjFigTH17Kmp2zq0eMXExqOu1ZcWxSaTkQHSAwQaYBZz0IJOKqeFQv7KyI5rXpWCFlwnAWZ43ZrFD7Td6Y1GFCS8VLohNooudZcUAgHN6HgAmrjOqp+IZZy7GubZPink2KipLXgJDty2dv9X3VdJ4JeeGdeYzY1TT6r/VG9MiROBNgeMx6K8ba+beWbIloHE0f+VG+Vto+rmxeVnRGcdgOZngceBAbZIF+mxxSLPNVXcwv4bdyo//y/x9tKpRymj/c6zjX+UDp0d8afOCbcw9c97NPfR/mP8bTfZx/JT5b0ttezzP6SAhjINDmr+07tujDG8MhCw0K+Za9IfC/Mll/FgZB0O89m9GI4n9B3u0NjTvah9xvcFssqzwNxfxdws5FOJb/5zffgeHg+z5ydS90ZmHccxlxJ+8/ZeBpIoYEgqM7Yu90sSbj+Dnb9Smi9/+G8SdY5j2rhOj6dzEm2pH2KzLQyVSmF++CT+/hw2T7/0lnnkHG6a+53dwZrNJQ5IcYmZm6I92n6EBEiZRFscXgoBq4rNLBIEWnIs/stSYnTN6dUQrRhxqZK2pdZ7vcbQuISJa2omeZUQkltU5fgCTh6Q4xAO7VqL0jv0DH5Nz0vAAgGCILACg0CgtnVo2lgWIi8I9UbcQ5L+X3nFUg0wAcBMgAcDdN1XvcyYimb0OpguvXiZvfnZe/oafbmxo6ezWGRWrahKvoPgAA2gD5R5mJrSVx92HpwuVY9oQiZcei8qFpuXDT2y/RUl4zjCAMyZcZ9cy40y1s+M/eUiCQ8w/2EECWlOoK6vt813ZRUzq+LMr3udNrLzA1q9Q2KDnC3D7c2SR+ZNN8cQg06VNCRiiszjkHlZMfPG7lwvDguWdP6AwmX+4BSuff6vSrmf/2uHQ5CFxGyPTt/b7NLEWgkMhW2S6bKFBIlT2KddYA6SFbCwSOuz9t78A3I9+9PSC/Wtwdt6eDfEKdp9dnchmRn1wmBRUyvPqGgDQDbNfXYuGiudv/c0bezLx+3suF6Slk8v1gYTlENP213xKwSBN7PIya1uyYFZG408fHHO8LX2hBOT+3T0/mvP9rR9W8/9+7eszfvapG3W8cs0VMQVHVDtctq9m6Ffyj7xNA8I8W/m1G7Zl/fBDt9rb3pP1YPvv/6PIUaknDwnLIQ58LghVtbj9FSnJEsyCCSy0ENr+zKyxCtu/vK0AYGp7omfzHVownfhvvntNPOox7d6E2HscUf9/ee2wZDP1L7kAkNymxdmfWq9fy8T0yu89b5nn7NBOIhLlENMLP/RalTdYPS8adza/YkqbtGAQwCJ0530JjhGDAC0ifxA3myJjX3Ti4KPuGmZ7x+uG+8fS4dlxLABMGNKSQ6HJRKJqAuOwyWagK3CccPCIAU0gJjAxkZnAdtSAOzQNmDUFmGO0PWBuooAr+vtX59dDLP/P4ZYEQm5X9I0DRisabECA2VGoJxWJ6kMUahGgju2ChaonGkjISUzMTC292WOWj/o39rAO+r5a7uHXid9tV55805ofRouujMvDZQzF+OgQaJKRMId6+8EagsAkCKTBAgOJ8BDoG8qhCaivdZcrCn0Ah6O7Rfp9euHbfzgiD6ypHYpccyS8LlNaaB2xShODQUxakxaaWGKYwXkCg2ov6jsZzCvJo5FOY/z41mcuVEUrNqLtVNKU1aV5qX6LMxsJc8iIJOmQCgyQYJtsZoYmmIA55Ma24+Z4JRH1tfpc4tiRzDJ31BUdWrXs59+adyLab83OTHrVzuLAep8jvFKIRDnEGb4uAsNmEBMUEzOzYglS5BpqqS7cMv7enCgONdOi8tz+M8PVHBZPLv15+C+sT/4iSiEK5YxDpmx5YavhkCh1SHRtr8XXTkmCDQAEwWBmrZkhhaCsvx2axm7c+hDT2ebyEh/Adu36EctxLWKs0E/ljyNZDAdfupUcEqUMCevUWHxM2sxMxAKKmKE0g6FButDHsRZEyYIwb17EhmRa0deYBSja3YnJP56kVeRdu+tmx2SUMiRqHyIsMZmVrWylldJaM2tmzWBAVaVsPBggAuDrjfbqEIN/DUPYMy6pl1vjBMpKHRLeiuTKahsM1krZyrbscEgrJhMAYXnKFtg08HdZQ0J3+xN0GRjxMOWFTlD+lCGJ7ewtiggAs4xIIM1gC1Lo8rmpHg4uuZyAWsU4N06/IOJ5phOUP1VImEPEa8u1cIGYFZg1EQlAkBT2FleilSTcGGW3J0AO7s4bJw+Il3ZfcjSiASTp1ByNJOSQ+WY2tBSCmAhSEJGQ0iSq2JJ6/ZQXHBtbENHpOeOeRInXXWhLda+nJxjEFtH4o/Ik7sdIvOa27S6lYROY4AkIIhIkrXe4J2FS8GZdLh7r2e2mW8bPXuKbnl+f4YgiQB875fcX1czJNa+eaU8KySgFzE/UupRWmgCGECwhZOCBmyZjmcy8fYzEikx7F+ZOpGVW229xbI2o/43fw4AlvVULq+R4/GSSUiwZT+8ULLRmELFkkxS96YZJsrS0nt0wes3ne5ZPrGkO7botjq1x4Ac5vt9l0v0YbOxaELrlR4JYiC6RbYd1dl758pyke5Lc4oTp3I5GJUBgqQUpY9briieJQkzHxKLR6u44OnE9rLt2a4w6IuHXghQ5yzTJQxtxlgsRyD35jY1sXf+4xSCG7nDlwhZQFlUtLC1O7teT5AKXCY1nW/otmwlmTsXcwslzEWR6LWt+fG/Zzv23p+B9t9RtGulgi97Gbkt6AB2At6DcmEQ3SCb4G/2W4QFUkHx5FWJKfS6Zjv/a6MnTMNp8HptIKhhhmzMrllUZiRM6aSNJ5EeqFAyJyXUyZap1LY3dAFPL8ZvkxNtmOte5Jsq9luuafOWFg54IvR31cm7RJD0nkz7dmllWMLiw6WltMmvyp5BF3PvjXmoucGnZluu2BIPALIQdoMzKWVWFCdJoHIa2AR9GxmQrC0wnOzbE2GllwtGe1CQQZjrOS4bVdKaxavaQw7sE9J/sWZ43OeN6omVuJYY11neqd2X2VJGI6dmDbtkdzlfycqFpCw1AEDMLUgooml9VaCbAo7Q21jK1H5xXFfW7ZELXgcpUndRgei2/+up/+vZVzI96aUwI1xprR61FEyHiYzmoH2sJJcZYV/r3V88Z2Vhof9byUUpF/qGrGstEAne3PQm4dH2hIZvKWDA0KUPCMogsITSHRXbZklmusWiU1hwCE040z581/Kfadspc6UrZT5Vp9wKrbgsTwFR/ZqM7htwDNZ7c6IlfhxYdL1atAtAZEqBggQ+wX+TNbowCprP1m4yYjV08u9nEVCD841aWgrqtIrtpljbINmxIEEgzMYMBoXTOrMK5+aPSKL05BDDxqZacksIBlUF3N7dlzc+YWJ3RLfymQpYVAkwnetbF078Cu9fFjUOixV99c93R/Nfy5bbfutgI1K7CM3eXiKb/vjc+05kOqdXxGuvdu8kXu1jrV3NsQLoezhafzQ38WRmYfnHahtH2oepxPPqJ//EokkJdzpeXy5RBIQFJxMQM2xVmASlASjNVzCkrEXHXaunOocjcdblTmQJgm7OLilOryTPVZrasOriZQSdDK+LXbD93U0a8Gh77WEsBbsjYgTvu/msAjI7C770bz9zdUhj3BB0ddI9iubB3bPLEvNh0/3wb4tzO9izzjYXfu1QulNzavt4ym7+8JPknt57sZ82CjTYutE3WRpgEkwQ0KQ0CSWIWxFpzyMirnjcrzlot/Tk0QBlbIxIRJsUKJ9PRnmWHfJUFaGhcN0rdrF64NfZKUMnsb70D6O/Ll7O/8C7AkuKRp06F4L7hpq/FrfCsf9VojYV2j+ZpWfyVh5gA0VAibGPeo28Z54P//rSw2RAKBEitCFqQhnYxKyUBAWELoQFNrCUrl6+6oKIwxgikZ5z84SCAIQ1EfgUpXrMQlvprs7r6Nvef3joaPUlueCm286No63nHT3YXv22BjZ7yf76w7P2Mp7exSXzvz6BjRzXmnsZRd4jIs2Zv7MAnirTxntkPKQm7jwEwGiqBcf2yuk6aIXYRS5AtT+V7kWFp0kqEJbFLMTNLSRoEIgiCyYHDbBYtrMwxopqbJuEwKPKrnBQvaM7ektXf3PvKGNYCyi4/E1sdaPJu+d9y98JfyBDf85r5j6V+dK4gIlp+OV6LNFZjnJd7IeYNUsjT//ULJQEhAECCnl5QuWFP8nY+PMNKSK1IU8Mrx+pP7q/vajPIdAmpWIJZQkIrwZpJSwJIKGGg4bmnnnr+fIiGus1PBzk0ySDwvIr925d6xriPa3ZUxdyjlYGaF4H/eHuwr+j76/HPSz/+3Yj4iZymitXiqXljvHfiJdsrYtoGlHz//bOvnk2nluDvnir71qbDS5N7aKaDF6Q2wAKiqz3cYWrR3h6AJ9OtbNhSsWQYsNkmUnCx0GA2tCYpVWdnbW7F/KyrWxTTQR+aAjA9u3VsT7r2+pWxxMf5uZdzDGX7LhcCzPz1x86uvecRJnz128djRuhn3nHrmHMPt7THNNJza8mxRQRA92U3lgh0/ugD0OL2gp/o5MIi2t8OsZaCWARelQZT2NBiToVHCDKUP8gEkCTWSmomlrAFacFCCxLQxGEWZcuXDLQ4TeayyQadnT82hbjAH0uy6HLuIobFpj4DrWVbNm79H7It+unWOGvhM/MT0OpK22NFwtD0zZLFQy/kfUBBqDsS8R0e+ii0Peh2GwQtlF8bWoU9qmLbfNnn7+7u8BtludJlSA2CBAuWtjJBhiYQYCvSwvCZTU//YqAyh0MAwGiqHHsMCHPPxZIMrvveZQjjc+U5Z+b/Xhqt//Tn+PDhXS7X3r0fjRm6mdFckUhjFQ2xGsPP3jiMgPWPSiss/1CR5Fqj94hbWQSi0L6TCGrDzlxxQ1uPgszQZIuuTF0LIksJzYKkJmIQC6G0ZmabNSkWnou1EeI6HAIACpojFZSR48ylbTG+lfoJv/eu0v/YqRc8ds+yLcV/+kG76l+3rN2w8SuLYlKFOnITGvDK+liyRR/bNnj6hQBo41MftOjLL/xtUpHGmZ5RYS0gmcLacLuUZajFlzPmwpxfXaAq585prs4+edbv1UJCSElug6QQpEmwRRFzgNnXJhojtTk6NQCgMcbvOMY4S7JivDCRcXTXhbx7AP7QAztC35unDf7AvS+qLRVxxrVtLC9fAACbUCPLi4vVqyM9I6vKIFDpsbdmilk7FyblS0ivns7UljCUONNBOpdbhX1jE5fnXvS5z8/tKjw3u6RtxS/NlrzLcmFGe5i0KzPbsAQImklqxYLscH/BoAbmcAgAU9fikd/9z7ryqO8Ixa3lsSpQN90EJUFc+BZACxCX3R/PZsPUURN15Up4uOHI6skdUZqqzg1+yjsHgLD4aIB9SW66qiMZNhnaBvvDEF2GK+zN7COhpWAG25qN/qLqRtWmjbpwiDQLw1tZKGyQ0MwKQtqiPcMdXhBp1ZnLAABBb/TvWOFDtSPnkszemMXlwJ8rrzQiKeI0NkK8hHsBLZqGf0kojHXwhO0rnRpQ8G2PN54RITaYtrdpFmQaRkAQaWXZTEKYYYYKS2m4FNsQFqA9qs/OFJyRE+45XttrkCCDYJik0GtlWIvnRbricAhArOGWyHOP/DozMJEQXcwAyI7az2f8trRTiUdWQg2/4A3G6ulVE9XAHGIQxZpNRjnqc/mAxzBBzH21vURCwPSqQxdaQoYSplJ22DDICHWScAfCOn9+Zv6i2YuI+o73G1KwsFhDWj0lhtg8IE6duQwAONqMw7V2Zt+JfLt0eEwI9oYSr3QEej0GwAiaUYEC9X3HZjd9+av10RsjLnsCm4N+XzxvVqaXTQJBC9EPEMhgBHCAxbGabB9nWMrORDDDH3CTbfuCnbk6q6V5tRCip74GBoGZmbozZO+GrAGWOhyKBa3vDIuevzHVX/7zcDWFJuK7edw2sioyMWLnVqjPmrOyL2WOmBMmsjl4yKS88lieBkwHz5iR6bSuj8DCCIbJF7SzVPioURHKOiFP5p20Cn/nZbO7KPNi0CazT4fgKzrZ2hkuWxh0M9Cls+xZVw73ORyKBSHagGWPbcWIyT7YNF7RwGTNC3XvQV52zIsxZh7qn0BjXNPbtVvkleZHh7Wg3r0uJk1EgRYYtjT6lKwoPJpVIlo71IXgEoNUe1b276GFbXo8AkL2VGSHrAJXti1lU1/f3EJPOJRL9s1XCO5wCEAMW5BSkkIqOloNhckadyI0oYJCePwBk6OmKC3//tGmr1Rcyo7iq9YTaEwHSXi7A9350U9K+/pdBEg2OhUTZIhN3VcsZl2iWZ2G1dZSLYj6fjFXB7UIh3Iyg539/aa47M3sDcmQMHrpfN3i/GIZXjj7yjtzOAQANGKsJKSmkQnQA4XjCZo1gJauvqyaEoGXo74Xf3j0kvfztLYuandtQo1dbO/NmV88cslElw+azEJo3VsnmA0RLKbWzkoDAcMOlhrNrcXdzc3a09snlSW1lRVogdFEurgnDGGbltCa24stwu0crdvPdIgR622JV2Ksy3rH2twfDXPNbIC1UNEX7mh3QX/uY4jSqXvG7/PLtMCbBfDIuVDvIyYBzZ76fgki7c3iNtbh/krYnlz2+P/Iwmtqv9ZS+fzIz1UhEAqzO/0yjOKuXoL2aOrbkHlVdDtrewAEn3+k4uGNsefeOZGwxAXZkTBv3r7oxgxAcJSexGgtmMATFWdxDH8rxrETghnQsq6JbWKtrZDHrbpwSQYuZBgBm3KyMgwmLaXkbtEfKMwo8mahkC2NoNkLLUgWgAqGesg5HAIAFMdwF4txiBqduRPylSEAhMKW6K8FYo235Z1wYyO+s/a62Q6HtdFfFzQMm0OkLM6ii2E+2UhuyRBhUipsa1YhF7rQdNLvNXI8Z88Gg13Z3NEnWRfnUGjzUEcqZy4DAC4+vTCBJRD1puJECZfsn5tIY12+BCpLFnWdbggmhRBMC1l95Pf4jbxgH2vDdGd3dLks2FZOpjuT+vqVti57TaPBbNXgdspnlpqEUWWr2cN2axwOAQDI6x8z5QiYzlalwp3bJfvHpgdTXSJeRsmBqYsIUpFCP9lwF/UYvSYFs6uP+IIQNT0qLPqzrbytRT2wvZ7AC02FvR1wSYSQneOywWChK7xavGmYi6XDIQAgXnh87djTuu5ekYJTJYSaU6vGrifcl/oz04QsJpsBA51aCPuS0LntJs6SLrlcfOlE0I2eQmvNjfWnbDYhfNsO7/VlUG43ayO7MyypsEOqvGrbWjc8Gq/DIQBgEvWLxwqKxnRsXkokA+cfD7rHuoeOLJmMcAYLj9VLYsFU0GpqZkmc19mdYbjOq37DDvVwQbhywwnOzqy0u9v7z6wM7XNRuyBDtRnQTFCq3ELOuuE9c3RqAKBTv6s+PKZreaCjPCXe58QrXhuzsZ5g4WS4uhvbSkNB1myVFVmGIUkF7UU5rRcvdYe6Wy6GsguV3HpZZ1SvKK/0zc7hi2syJJGG0m4SgppsXZxvqDdEzcTy85PQ0ekFpu79zcbN/SNMutG37V6XqtB77q5wzqg3MO3ekILYODFgLl9Q4PfDQKGvP2AJJjNMmZk+F1fowkxJwrOiO1ziO+1znc6UnZx7NmgIwVqAhG1q4V7Bqnp9lIB05jLQ8Z4CysLCnfmjGX+YDlanaKHExMtfyM8Y9Y5XF44124279aKiNR21l+tleYW/saMPyiaGi1zarQkgBcgQZ3ngIWbFDMuAhNCCpXKFq2Abr4vm9vSQQ1d8GFIfa45pR1Zli2uei2e/VG6MctuZ8KIU6bikBVfurpBxb2A64pq0NMYEsG/u8vnc2SMKKwrIsoUQQogwg0EiXFjUFc6c7+aq7jqVb+8XEJKhS3skhM6dp+3lI44sTYPzZQwCrH4LbHi8SH3std4DatOrG5k4/OLGjDiVM532r01R1KzwweUeoP/Fre44FWpxJLxmsig00AsQgu0Xj3RLKfta23qIJJgB6Q2E3qEuGxk+CvptX8nPAsjvkAw1qxnQWONB9oMj4tqkPYeYYDe093tdHlAo3G8UlPtSyyJWL/l61mYRgPCupSUxzwUSDvDqVLTKhNNH5y1nAkK7V+XHaeyVjCWTS6GBdmCdO1UfhCnam3qCRAbZ7uwW8m6jrj4QmXnGrztNUdQqWTNBsF08n/i+kfvAac8hdJ8OzSq54gSjmhutqlkpZBHTC+syzwzaXV8xV42om0F9r8ypSk2Tl853y7si2xr8Uu7SWI11v7Zg1tRE02MQwg1HLgbYhb7W1n7J0lDC7l62JtNQLn/tQY9bsYSALUBCmWsEyu4bqeqnN4eYQgdoUfYVPYgBQriudUnhKNGhbBNgxYAQBChBowW1Y9o792p4ZKb603Orhk6XDIJ9tGetNyWj2ltb0tNfMWewsYt1C8ujGwsfDa5NXZS3BN4wEGisOxmQHruzu7WfDAn02tkui/tVJktmIpAWpEktLCDvAzHM+RPhUOIKrh4wREX8xIkwkKN6zAesa1iVHR2PEWQdEKPHR7x60EaLoL84vthiOuwduvfDhGNtpbOHenh0ng0sSOhA2NgI/v71h0suDbqyg0mfaKsoH95YcOE4MkNOCExA8PQRf59whTpb/RZJttxZrS6GhsGR/JXsc7flrrTV+lgxwcfPISYoyHEWVhKxTlRFt7AvY2ms8WdqOLUp9tpXi2///UUlf/sgawQ//Yi2733aZ33nnXFJdL4ratuBCfVNobycDI+AFehv78+qyk3Z1Nn9wqaujFlDG+NLzVZejs9LCId62oPZc7KmNED11YdG4PyhJmiP6mxtAsOlQFC5KgjLZCZII7jebWU+FOutT0AOnT109O6E1ipK/mVb9qcXgOmHvw6ytP5psRZf+IW6659GP9/LtGN+ebxzfv17Nsay1jAfXlXaZBtPPfJcULB7lrileVfRs3e9tipON9tPbB7RAhPQ1RkMAcKVUeBL3UKQ6SD7jeGxqZjAnV3BELH0+Ao9kxvxe7S+gYD2i+eag9Ls627vtTQEVI6nWQKmDJHSxYso/OaaWGXHbWP0/+olofoTupVQuuzD713ATD/reCuLxiwtth7/puvTf9yvRpFjTDuWxlN7CL6bd252j7yowzd++nEwzsypBLTA+Z2N+eE7P/HeA7GDuAQPxgjiQmDk5g55uSmj0AHvwq4ovx4CI/+KgTx1jSULAsAFBat7jp/poKJi1XG512KjL0AE7XNfJmHOZT2rJuZ4jFcONT3WMcvf+0DCyZ9EU5FQ8qY3/w0AYMfW7mzA/eT98QswvTKrfLQaQy/eMaJxJZfd9Nk5Vtj1wKyP7ZmzHvjO3zYoyftu1CrGj4X5hZvj716kOt8L0wHfgri5I6YkuczYfQSBu8+cbGE3WZc7ekIwoaUmqUPLC6T1QGnMUuPac2Ud/JfexX1W4dIEKaSYFcC4sAQWYOOJu7KV1h96HCpuETrnHX2H071qb/QPQMn30b+1SjD6frDu668rr8ehGgCooq4YYojpxRvM+P2n1IqEUSmU6sbGCyIwcm94530bc5QqXbZibonQggUrLMijvqWlsQckaQ4xA8S72ud29vHrCymxo8ORF8TwP+lxex438NKNEEKsPxK/NIfOLxsjZGFhRlSAHpbfeeplNgETtVtaXupeczfaDACcE+vcMNO++aNvfKYSTAe8C66RrpMUiMBcveldD6w2g745y9fN80IhZ2WVCmdujiMrk9WHmNDecL7pfBG1uqr2n6iaW524FiiD/pZLRX98XdaDoRoAyGiLyyGmA2PlRSNetqNs+M/32Pt+dF7U6VPn77wkFPGXlumFxxkQDfCNCNDKdCynfMoGlemANxFv27QAAYySks3tx04FaFZ5SJsuHTZ63hoviXdyHGLig7XdLL0+aq/2EPoP11bclZuwTiR/9QYK3/HXjzyICwBg5cdVAqhHZI0l4QjVZxYM69zROR8NQ1mbcs9oQcQmeN0XSAtd6/WMiIxBDf2Tlb0v1ns7IaYNhYAIjdzl5bdcOnOpQ3oIzPYts+M9QHKBIKn5u3/sd7lNf3uj5WMAhrf1e2M7bw3CfCPIpVcHsPo1zcx7axBXxTyZgBcfVzcPnaQIbzvb3NFx0tt2JlT1bUHyiwvkLaGfmCQ/8mcjDVldZ6aUQqFUeNFOKQhgY+6dD28rV1Zf2Pv2+AG1k5FDTPtfdPlYCyJDZjErAWLhfqbl9gTeDwFo/l9PCC1+sBzveZuAFv/2xbgUtsMZY9dJKLpcMvQLrQHUBWzb8w8PP73ihR0H2fvUA7+Z/6T5aLQ5k639t00phVZONwoBA5Pa4sWhPtuXOYrKksTanvHsCRdsQQwIhhJM0CA2AovuGeMNaRJn5pIOVlf8e/5jXz22AItyv5Px8f0X4tq5L9pzE+lS4MjI9Aj1P/uYJjrynUvLP5oDpsOP19/xkREUwgsbPQ6FEuw/aHTTZ+IcYvrjUbceOH5LGqRBBC00jMDWNWOUDWx4YjWAnk/8yFr12FpG6EM/4tv/K1fEa+qVFa7hndYQAEcZtpn23Bh7yotkSSWKfIg2MDLtWjK642sKMd0pNDYS5hDTyafdGiBFYMBL/RHbJgtiCr87sTPEQzbIR7/tpZHZKrSIsSvx2mJvjNIAwDywZrvyYWixgqrJfrFXGzsVuL4plLhOTeG9BoNIR8RQBgGaIAAi1mL3WHaiSBRB0palhn2I01ZUwHHGlzaJMH19C4YF6CBkd8coHfl70C2AxAgKnXZXTZXLC9OFzuucQglziHG4SwDFOSwAQbK9j0gSsRA8ywujrnWM1zSwpyBMUw7/EN0OA0AwKrwG6Y/3bHH92yeeiN6lzenhpEHN7Yna1ycMpgtNG65zCiWxLjslNXGnIk0MBgsGMRPAXRaDTxel4k112YUAA7aMUnOEcXDjmgt1I8SH20q+Vf+JrVNHoYvNqUlenM5IlEPU3UWkuV8yMTNIAAIkbRD5SbBsSrCe0dEUqDPzy9xQI9xUWL7hc7dV2dHd7W4IqgTrHoTs2TKFFLr+pVAScqgx5GIQgQZUVGImUdbeRxDCJuG3Bt39+1rG65jGorvQ23v5iC8vI1reaPm5R07ed+ez0Yv0nNLlOtkxmkI/04uN178USphDTJ0R/wTWBLBgYrBWLRZpwUygQGiQQ8I97vPXQrqE3W9k5pr+qObFNx+5NOvI4gefiiJReByux1MphWYChRKXQwqawICENrwBxQQQB+RgpnX7yoaUd1aiVY5Ex0UjZ0UeIdQyfKQJf7KtgnXtiWgh0jtWoIVrCGo9f/NMoFDiHGIIjmQgQnZGH4GZmASDBTGIXWOn/0oAs+d4AGa4w9FXygEi96rhHGLqqkm88qkFU+vxmUGhxDnkYobyiiBp0e1nJiZQRCkCwFzgS8X7ygATCIxYKXNiaTKBlDQ7GZg5FErUPkQohyBRkqmZQIrBIuKNRmRrMIXLE6sngYYAEIqaEnn9ZFHaxr6ZORRK3MZYVqg1NXVIwawjSlBOhQSIiRlAiuO+zWpOyEGyvjQt3JBjYAZRKPG9DnO+JchiaCISRIIg+xQGDhxyUXFKNw/YG7ISuAsNCSS1vCZom0EUSphDhGpDMDETgxkSEB2dkRpKNcILUzunEOadSGA3uCMzTcMn+Q/MIAolLoe4dHVoQN9lYiaoQWNjL1H+6hTPKVzaMaYgYjqyOD1Hyv/K1Dm4pQES37fnDXmKGEREQkQiIjARSPWSdY+ZaDUJt7aydixBRBeK3Gk5Uj2v3pYeR32mCInPQWS+IccWBNKywuRIxkkSxARxf1HqO5brahx9HDhUtzgttaGeV25NV01/cpDUOVfr+ZMQABlK6MiZSALbWffmT4LkZnphvY9Gu/781LmzJgP/q1tnlBRK+qx040tNSkomYoBJgC1acFuqJ7IIWG2/Ob7xm2nX/JIkapsyzDwKJckhJrSdbOgMggXAgOktX1UyWeojh3Zv9sStfFdNaVK1TRFCO2+faRRKOmYDE9DT0dYbEkTurJICYzK3wfv2rojp2sYU3LMkLaVQaNdNrplGoXHE/RgW/Sz1sX6v1EyBfRs8e92rR4YsJJy/sN47vmonF8HdN01WWOk0xvhixwwWmryfHNOF2jkrtWg8WTU3Oj715RNFi6f6PSWE4K6bU+K+MM2QnjE9mYIHOuw7PWBCXUN+debVS8GmhqzFUxj0MgmEXpyRFEpTDuFsU1G7eQNHDli2XQiZ+RkmoPs7QqKkMj336jm8a8uMpFBacohpn7vmUEZl3uB/gVBnn00gX66PJlEHm1Cn9XM3p6fZfNKRjhwC9x4N3LLnarTNIZHm0pNAAOsd6zMnXs20RHrOC1kqc/vCq2wZEmkuTY0vzDvWzVQKTXHuKQaNjKAw8i7aubTgRGFa6s1xeszb12Vd605cM0zpXMbU/Uxg5aoxYpsz1RbMnkYEAljvmMEUmtK5jOnnuV/6/ppbSY9+12lzNqelnhavxzObQlMqh3RP7k/fiqD3x/ePelvz+el1so/puXVjJza/jjGV+pDwvbYypHxLL412E/dOXUiFlIBp58oZTaFrsLavXXNkySi5oPT2rWMnFEojMO1cPAkueNMJU+zUfupunPvZ0vi8ZdqxwaHQNMPUyiHuflH8+gd7F8dPJ/bS/Gk1Ikw7l0x1vrG0QywOXYnoM4ohZ4iRJ1nt5eEj++KYiJgO+yYtn/JkwJFCQOy1vRwEAGgACozjHQNXmZkZEoHBbBv+YGJNMU78GpatNjTFO8NK59R0o9Aih0Kx9KGW+90KAAz3v1ZAi/PP5b4Fynjj3z0UuRwZ44KWl29XWgJKrPnsexMc+J1/cWYegl+6LZ5Vqq1x8/Si0J65KcrSOa0Rg0M5H5cMAD0P/lBDfPjJ2xsf2rsEhQOOg6e+aOqM123rYiOiB2h57jh0IqHPSP/5gZpF2fvWfSu2oZqDh6bV0T6mPRUV06nDk4WRHNLmvZEPB1wey/3sf/YK/HxDD9TA/JO1kOnLZds0WAHQ2nzb2x5/aJktE3iXQv/r3+0Nr1gUh0K8O+GMeukApj0VldOpw5OGkRy6MsB/vFdKPPkBVvZb7j+5ZPDbsk8D39h45ebAtvDze1c8+c6EGhO69N54e65MO9dNziGjyYFDoSuIFgkan6yunr+iqrpqxSf/p/pjKD4t2WXroZqj7m1cBTS3C+Dc/UU3PK9uPPeF0sd0IkYCAcRZ7THtWzB1GekmDodCVxEthwQ++Db5/Fd/1591xz/eZRfyZ0off6jh3UMPQzN9f3WOjRuDRSD3xifdLFF1bN8rIun4rEPBdCynbBoNCdPLsxwKDWDkXFZd3ZwjS/ozRU5ZKbjg2Ps/nPv+L1pXpxllfGS3NtCw9w3w991/lkgw6/x7z9RM5JXSpUBCac7TBEy1+dPLPWUyMZJDStyG3JuAwq98kC2D528HoK/eZhv33XdD2IVgiPDq+z3M4kRpDhhFuwYXZ+OwfFPXuekUsIepNmP+NOrvJGMkhySORT6Ec0DKr0KarECbN7LeBxt/efiUEoAUwB3nAGDOD28cKBjBON5t6NVE0uilC5hqMx0KXUUsObSwzQJAOgj5h4eNMJkuaW4xNQBocWfDKS0HglHbAJQMd9gAaJBCHaeS3scl/5Y0dbWPBYdCURg53kx1+wcD+9Cd9YNfr5MAIPCflVoCkCACSYCkEMbQN5q3LvleTKetemcii0YsmaEfLwIAEf5AFQ+4rSojPLATWzloIVSBgdv7h7u2pm+435SA6YBvWiSqnzrEsDHyo32RuUrqKzMUiT+ZM3gdgIFuc9bDIACE95UNn4iu6/c77bKMTwHG6z+kr29pEw/Xf3LWcSAWh658NfRd6RFZUQdzGuo0PTc4CXAoFAvT6QzONYdDoZiYmVPS+OBQKDYcDiUMppNBh0Ix4HAoUTBd6FrlUCgGHA4lCKYLzTMgv+944HAoMTBddCgUBw6HEsIMyTI+PjgcSgQOhUaDw6EEwHSp0aFQXDgcSgDUem56hbOZWjgcSgAzKTnrOJCmySzTCpdPOBQaDY4cGhMOhcaAw6Gx4FBoLDgcGgOtDoXGgsOh0dF9aItDoTHgcGhUdO+fYfl9x4Npuy5TQ06DqMk6GeJ3KJQAHD/GUeDff8vM8fMdP6bpXGZj5Yq1uyOe3zZWr1yxaxIa6X7VoVAimKZzmcSjGVvckXlG4tHsTZOQ3tW/f8ZlGR8fpimHCHfpwfMnhNfBTH0q3tDLtzoUSgjTdC4DtLr62QanXK0L7ro5kfiADqatHJp0zMws4+PDtJVDkwuHQknAkUOxENy9ZTrFF73GmO5yiC098Uqi6wzt3mI6ulDCSJhDI7RWfXXwWMO+Rl0nUwCpNVOz3rnR5VAocSTMocF1LisArAFLYeATSCA85fZu1grAr+gbABAMpbDeHRt9U/0w0xoJcojx/BMRewxJACQ03vFZaED1gfHdzyPj3BSTiPDFWQAyanIBgS+kLPUK88xNVD9OJCyHfv8NaAB44Y1QePK9Apk+EPDbtVC4eAo5U61YkevTlQBuP/0uQOAzs1NULfP2mZuofpxIcOg1GvojcqjnvNa67RKgInMZg5kkrKnvOgMA2/rK51TUqWdyovpxIsG1PQd+qc5XAABMIZB1dcwYJnzXZPOfAICMq58njpmeZXx8SIxDtrHtL9ZubrAlwah7S8A4s2Dwir8ejza/uPpaP0ZqwLR9rUOhpJEYh4w3Nv7G+NWiAwag8x/qc/2qLfK9FvtCgRJZeH04ITHtWJV7rTsxDZGIPhT6qTvrkIGfvMn7L13g7Df/6Vs3hCNXWP+y6Nfv/sQtgQRqmQbYsbTgWndhOiIBOaTpP5++VT9y/juPvu/D7wRY2UY4on9o+eOMn2+7X1tD1ZFA+/S0fTNal874FNHjQgIcEvJphGWwn0Pzn2WGkBIDOwGi5+0HVtZ86Jtq6O3amp4cAs9zdKFxIRF9SAIumIaUAMF9qMoSPXcDADqrP79SbZ91e+ZQfShjTgJVOriOMDaHuHeN0pDN4ZcVZPjZ29skmN1aAJCPv1NL0eD+h0RSAju4XjE2hyhzDwMgMABdIPMAAAzNyHmnFiBTh/qupAx2MPOQwFxG+UP+IwbMeYy55WAIAAIfDmKds005Y+GcL3MwUYx/DaWHOX+p8VbjYNrDkUMOJoppastxkEZwOORgonA45GCicDjkYKJwOORgonA45GCi+P9c29wFc11QtQAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAxOC0wOS0xMlQyMDoxODoxMCswOTowMJ/qHr8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMTgtMDktMTJUMjA6MTg6MTArMDk6MDDut6YDAAAAAElFTkSuQmCC)\n",
        "\n",
        "**순전파**? 계산 그래프의 출발점부터 종착점으로의 전파\n",
        "**역전파**? 계산 그래프의 종착점부터 출발점으로의 전파 (미분 계산에 중요한 역할을 함) \n",
        "\n",
        "계산 그래프의 이점? \n",
        "\n",
        "1. **국소적 계산**<br>\n",
        "전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제 해결 가능<br>\n",
        "2. 중간 계산 결과를 모두 보관할 수 있음.\n",
        "3. ** 역전파를 통해 '미분'을 효율적으로 계산할 수 있음**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYPVLa6W9PLW"
      },
      "source": [
        "### 5.2 연쇄법칙(chain rule)\n",
        "\n",
        "- 원리 : 합성함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다. \n",
        "- 역전파가 하는 일은 연쇄법칙의 원리와 같음<br>\n",
        " *역전파의 계산 절차에서는 노드로 들어온 입력 신호에 그 노드의 국소적 미분을 곱한 후 다음 노드로 전달*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M9MA55SNtAD"
      },
      "source": [
        "### 5.3 역전파\n",
        "\n",
        "덧셈노드의 역전파: 입력 신호를 다음 노드로 출력할 뿐이므로 출력값을 그대로 다음 노드(하류)로 전달\n",
        "\n",
        "곱셈 노드의 역전파: 입력 신호를 바꾼 값을 곱함. (순전파의 입력 신호 유지)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SifK9bH2OYAY"
      },
      "source": [
        "### 5.4 단순한 계층 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8fo7v52OdjC"
      },
      "source": [
        "#### 곱셈 계층(MulLayer)과 덧셈계층(AddLayer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs3mD6ox0COl"
      },
      "source": [
        "#forward()는 순전파, backward()은 역전파 처리\n",
        "\n",
        "class MulLayer:\n",
        "    def __init__(self):  #인스턴트 변수인 x와 y를 초기화 (순전파 시의 입력값 유지)\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y):  #x와 y를 인수로 받고 두 값을 곱해서 반환\n",
        "        self.x = x\n",
        "        self.y = y                \n",
        "        out = x * y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):  # 상류에서 넘어온 미분에 순전파 때의 값을 '서로 바꿔'곱한 후 하류로 흘림\n",
        "        dx = dout * self.y  # x와 y를 바꾼다.\n",
        "        dy = dout * self.x\n",
        "\n",
        "        return dx, dy\n",
        "\n",
        "\n",
        "class AddLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = x + y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * 1\n",
        "        dy = dout * 1\n",
        "\n",
        "        return dx, dy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyzgEjFiPuIC",
        "outputId": "97ad36d7-b383-4cba-9ef0-ac219bce5b71"
      },
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# forward\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "# backward\n",
        "dprice = 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(\"price:\", int(price))\n",
        "print(\"dApple:\", dapple)\n",
        "print(\"dApple_num:\", int(dapple_num))\n",
        "print(\"dTax:\", dtax)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "price: 220\n",
            "dApple: 2.2\n",
            "dApple_num: 110\n",
            "dTax: 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0SWdU_yP44E",
        "outputId": "c415afbe-6c3e-4214-9ea4-990ea57c80b0"
      },
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "# layer\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "add_apple_orange_layer = AddLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# forward\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
        "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
        "\n",
        "# backward\n",
        "dprice = 1\n",
        "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
        "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
        "\n",
        "print(\"price:\", int(price))\n",
        "print(\"dApple:\", dapple)\n",
        "print(\"dApple_num:\", int(dapple_num))\n",
        "print(\"dOrange:\", dorange)\n",
        "print(\"dOrange_num:\", int(dorange_num))\n",
        "print(\"dTax:\", dtax)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "price: 715\n",
            "dApple: 2.2\n",
            "dApple_num: 110\n",
            "dOrange: 3.3000000000000003\n",
            "dOrange_num: 165\n",
            "dTax: 650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H--RvV9O9X7I"
      },
      "source": [
        "### 5.5 활성화 함수 계층 구현\n",
        "\n",
        "- 순전파 때의 입력인 x가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘림.\n",
        "- 순전파 떄 x가 0 이하면 역전파 때는 하류로 신호를 보내지 않음(0 보냄)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1db-5bHRmVY"
      },
      "source": [
        "#### ReLu 계층"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQNgTWRXQwIL"
      },
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None  #mask: 인스턴스 변수. T/F로 구성된 넘파이 배열 0 이하는 T, 그 외는 F\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5tRr--PQ97l",
        "outputId": "bae14686-98cc-4165-80c1-91b32fd75938"
      },
      "source": [
        "import numpy as np\n",
        "x=np.array([[1.0,-0.5],[-2.0,3.0]])\n",
        "print(x)\n",
        "mask=(x<=0)\n",
        "print(mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.  -0.5]\n",
            " [-2.   3. ]]\n",
            "[[False  True]\n",
            " [ True False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnO6H_Ii0YDe"
      },
      "source": [
        "#### Sigmoid 계층\n",
        "\n",
        "sigmoid 계층의 계산 그래프\n",
        "\n",
        "![sigmoid.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WClAol_yR17Z"
      },
      "source": [
        "class Sigmoid:  # 순전파의 출력을 인스턴스 변수 out에 보관했다가, 역전파때 그 변수를 사용해 계산 수행\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSxPXQYhSqZu"
      },
      "source": [
        "### 5.6 Affine/Softmax 계층 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUHlup3PS2DR"
      },
      "source": [
        "#### Affine 계층과 배치용 Affine 계층\n",
        "\n",
        "신경망의 순전파 때 수행하는 행렬의 내적은 기하학에서 **어파인 변환**이라고 함. <br>\n",
        "어파인 계층? 어파인 변환을 수행하는 처리<br>\n",
        "- 입력 데이터로 X 하나만을 고려\n",
        "\n",
        "배치용 Affine계층? 데이터 N개를 묶어서 순전파 하는 경우\n",
        "- 순전파의 편향 덧셈은 각각의 데이터에 더해짐. \n",
        "- 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야함."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5MhAYwSSbKC"
      },
      "source": [
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        \n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKxtY0GQ0isV"
      },
      "source": [
        "#### Softmax-with-Loss 계층\n",
        "\n",
        "소프트맥스 함수는 입력 값을 정규화(출력의 합이 1이 되도록 변경)하여 출력\n",
        "- 신경망에서 수행하는 작업은 **학습**과 **추론** 두가지가 있음. 일반적으로 추론 시는 Softmax계층을 사용하지 않음. 신경망을 추론할 때는 마지막 Affine계층의 출력을 인식 결과로 이용함. \n",
        "- 신경망에서 정규화 하지 않는 출력결과를 점수(score)이라고 함. 신경망 추론에서 답을 하나만 내는 경우에는 가장 높은 점수만 알면 되니 Softmax계층은 필요 없지만 신경망을 학습할때는 Softmax 계층이 필요함\n",
        "\n",
        "Softmax-with-Loss? 손실 함수인 교차 엔트로피 오차를 포함하는 개념\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmblh3m0RzkY"
      },
      "source": [
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size  #전파하는 값을 배치의 수로 나눠서 데이터 1개당 오차를 앞 계층으로 전파\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpznoAYYW68d"
      },
      "source": [
        "### 5.7 오차역전파법 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7NDA7f10jW_"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "#from common.layers import *\n",
        "#from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {} #신경망의 매개변수를 보관하는 딕셔너리변수\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) #1번째 층의 가중치\n",
        "        self.params['b1'] = np.zeros(hidden_size) #1번째 층의 편향\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "###################\n",
        "#####계층 생성#####\n",
        "################### \n",
        "        self.layers = OrderedDict() #순서가 있는 딕셔너리 변수로, 신경망의 계층을 보관\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        " # 각 계층을 순서대로 유지\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "        \n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "        \n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "#4.5 학습 알고리즘 구현하기와 공통되는 부분이 많음. 계층을 사용한다는 점이 차이점이다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvbrV8XpZ974"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def identity_function(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))    \n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "    \n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x)\n",
        "    grad[x>=0] = 1\n",
        "    return grad\n",
        "    \n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, t):\n",
        "    y = softmax(X)\n",
        "    return cross_entropy_error(y, t)\n",
        "        \n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "def _numerical_gradient_no_batch(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
        "    \n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        \n",
        "        # f(x+h) 계산\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x)\n",
        "        \n",
        "        # f(x-h) 계산\n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) \n",
        "        \n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        x[idx] = tmp_val # 값 복원\n",
        "        \n",
        "    return grad\n",
        "def numerical_gradient(f, X):\n",
        "    if X.ndim == 1:\n",
        "        return _numerical_gradient_no_batch(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "        \n",
        "        for idx, x in enumerate(X):\n",
        "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
        "        \n",
        "        return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzYNkgEEY7iY",
        "outputId": "120fd9fd-5de4-41b7-92a5-5d2734f32fa4"
      },
      "source": [
        "#기울기 확인_오차역전파법의 구현 검증\n",
        "# 두 방식으로 구한 기울기가 일치함을 확인하는 작업\n",
        "\n",
        "import matplotlib.pyplot as plt  # 그림으로 보기 위한 matplotlib 라이브러리 import\n",
        "from tensorflow.keras.datasets import mnist  # 라이브러리가 기본으로 제공하는 mnist 데이터셋\n",
        "from tensorflow.keras.utils import to_categorical  # one-hot encoding 을 위한 함수\n",
        "from tensorflow.keras.models import Sequential  # 레이어를 층층히 쌓아가는 연쇄 모델\n",
        "from tensorflow.keras.layers import Dense  # 완전연결층\n",
        "from tensorflow.keras.models import load_model  # 저장된 모델 불러오기\n",
        "(x_train,  t_train), (x_test, t_test) = mnist.load_data()\n",
        "\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "#from dataset.mnist import load_mnist\n",
        "#from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "# 각 가중치의 절대 오차의 평균을 구한다.\n",
        "for key in grad_numerical.keys():\n",
        "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
        "    print(key + \":\" + str(diff))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1:2.3780531648616902e-07\n",
            "b1:6.525980362712949e-09\n",
            "W2:2.8862980912364287e-06\n",
            "b2:3.1159251607515424e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Icfjj8WQcYZ9",
        "outputId": "3be4b35e-4b85-40df-a844-bce5ff7dfd2b"
      },
      "source": [
        "#기울기를 오차역전파법으로구해 신경망 학습 구현\n",
        "\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "import numpy as np\n",
        "#from dataset.mnist import load_mnist\n",
        "#from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "(x_train,  t_train), (x_test, t_test) = mnist.load_data()\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
        "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
        "    \n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09915 0.1009\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n",
            "0.10515 0.1034\n",
            "0.1131 0.1141\n",
            "0.1131 0.1141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z78Z9wIDdA1U"
      },
      "source": [
        "### 5.8 정리\n",
        "\n",
        "- 계산 그래프를 이용하면 계산 과정을 시각적으로 파악할 수 있다.\n",
        "- 계산 그래프의 노드는 국소적 계산으로 구성된다. 국소적 계산을 조합해 전체 계산을 구성한다.\n",
        "- 계산 그래프의 순전파는 통상의 계산을 수행한다. 한편, 계산 그래프의 역전파로는 각 노드의 미분을 구할 수 있다.\n",
        "- 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산할 수 있다(오차역전파법).\n",
        "- 수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 없는지 확인할 수 있다(기울기 확인)."
      ]
    }
  ]
}