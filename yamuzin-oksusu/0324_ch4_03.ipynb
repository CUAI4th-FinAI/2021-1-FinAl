{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 수치 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.1 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미분: 한순간의 변화량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나쁜 구현 예\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x+h)-f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반올림 오차문제가 발생함\n",
    "\n",
    "해결) h의 값을 10^(-4)정도로 설정하거나 중심(중앙) 차분을 이용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#개선된 예\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # =0.0001\n",
    "    return (f(x+h)-f(x-h)) / 2*h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2 수치 미분의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2+0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x= np.arange(0.0,20.0,0.1)\n",
    "y=function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(y)\")\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9999999999908982e-09"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 계산한 미분 값이 x에 대한 f(x)의 변화량\n",
    "\n",
    "numerical_diff(function_1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3 편미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수가 여럿인 함수에 대한 미분\n",
    "\n",
    "- 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구함\n",
    "- 단 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수의 값을 고정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기: 모든 변수의 편미분을 벡터로 정리한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    \n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열 만들기\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        \n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3,4)에서의 기울기\n",
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (0,2)에서의 기울기\n",
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3,0)에서의 기울기\n",
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(책 129p 사진 참고)**기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 줄이는 방향**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.1 경사법(경사 하강법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**경사법?** 현 위치에서 기울어진 방향으로 일정 거리만큼 이동하여 기울기를 구하고, 그 기울어진 방향으로 나아가기를 반복하면서 함수의 값을 점차 줄이는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    \n",
    "    x=init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        \n",
    "        grad = numerical_gradient(f,x) # 기울기\n",
    "        x -= lr*grad \n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문제) 경사법으로 함수 최솟값 구하기\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100) \n",
    "\n",
    "#거의 (0,0)에 가까운 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.2 신경망 에서의 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-39076bb73c7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpardir\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 부모 디렉터리의 파일을 가져올 수 있도록 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'common'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 학습 알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망 학습의 절차**\n",
    "\n",
    "**전제**<br>\n",
    "신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련데이터에 적응하도록 조성하는 과정을 '학습'이라고 함. 신경망 학습은 다음과 같이 4단계로 수행\n",
    "\n",
    "**1단계-미니배치**<br>\n",
    "훈련 데이터 중 일부를 무작위로 가져옴. 이렇게 선별한 데이터를 미니배치라 하며, 이 미니배치의 손실 함수 값을 줄이는 것이 목표임\n",
    "\n",
    "**2단계-기울기 산출**<br>\n",
    "미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함. 기울기는 손실함수의 값을 가장 작게 하는방향을 제시\n",
    "\n",
    "**3단계-매개변수 갱신**<br>\n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "\n",
    "**4단계=반복**<br>\n",
    "1-3단계를 반복함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.5.1 2층 신경망 클래스 구현하기## 4.5.1 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-63d7241ae6d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpardir\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 부모 디렉터리의 파일을 가져올 수 있도록 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'common'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기계학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나눠 사용한다.\n",
    "* 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가한다.\n",
    "* 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다.\n",
    "* 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다.\n",
    "* 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라고 한다.\n",
    "* 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다.\n",
    "* 수치 미분을 이용한 계산에는 시간이 걸리지만, 그 구현은 간단하다. 한편, 다음 장에서 구현하는 (다소 복잡한) 오차역전파법은 기울기를 고속으로 구할 수 있다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
