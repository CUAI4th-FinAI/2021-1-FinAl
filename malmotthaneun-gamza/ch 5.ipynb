{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch 5 오차전역전파법\n",
    "\n",
    "수치미분은 단순하고 구현하기 쉽지만 계산 시간이 오래 걸린다는 단점이 있다.  \n",
    "-> '오차역전파법'으로 가중치 매개변수의 기울기를 효율적으로 계산\n",
    "\n",
    "수식을 통하여/계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 계산 그래프\n",
    "계산 그래프: 계산 과정을 그래프로 나타낸 것. 노드, 에지로 표현. 국조적 계산을 전파함으로써 최종 결과를 얻는다.\n",
    "\n",
    "계산 그래프의 이점: 국조적 계산(문제를 단순화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 단순계층구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 곱셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y                \n",
    "        out = x * y #곱해서 반환\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout): #각 변수에 대한 미분값 구할 수 있음\n",
    "        dx = dout * self.y  # x와 y를 바꾼다.\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout): #그대로 흘려보낸다\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 활성화 함수 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 ReLU \n",
    "입력인 x가 0보다 크면 상류의 값을 그대로 하류로 흘려보낸다. 하지만 0 이하면 하류로 신호를 보내지 않는다(0을 보낸다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None #mask: true/false 로 구성된 넘파이\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Sigmoid\n",
    "순전파의 출력 y만으로 역전파를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Affine 계층\n",
    "\n",
    "신경망의 순전파 때 수행하는 행렬의 곱: 기하학에서는 어파인 변환 이라고 한다.  \n",
    "(가중치 합 Y = np.dot(X,W)+B 처럼 계산)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 배치용 Affine 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3 Softmax-with-Loss 계층\n",
    "\n",
    "출력층에서 사용하는 소프트맥수 함수.  \n",
    "소프트맥수 함수: 입력 값을 정규화하여 출력.\n",
    "\n",
    "소프트맥스 계층을 구현할 때 손실함수인 교차 엔트로피 오차도 포함하여 softmax-with-Loss 계층이란 이름으로 구현."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None #손실\n",
    "        self.y = None #softmax의 출력\n",
    "        self.t = None #정답 레이블(원-핫 코딩)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: \n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 정리\n",
    "\n",
    "\n",
    "\n",
    "1. 계산그래프는 계산 과정을 시각적으로 파악할 수 있게 해준다\n",
    "2. 계산그래프의 노드는 국소적 계산. 이것들을 조합하여 전체 계산을 구성.\n",
    "3. 계산그래프의 순전파는 통상의 계산 수행. 계산그래프의 역전파로는 각 노드의 미분을 구할 수 있다.\n",
    "4. 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산할 수 있다.(오차역전파법!)\n",
    "5. 수치 미준과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 없는지 확인할 수 있다. (by 기울기)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
